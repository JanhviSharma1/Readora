# Readora
This project enables users to **upload and chat with PDF files** using a locally running **Mistral** language model. It's ideal for research, summarization, and understanding document contents without needing internet connectivity or cloud APIs.


## PDFs + LLM Integration

- Upload one or multiple PDFs.
- Ask questions about the documents' content.
- Get natural language answers generated by an open-source LLM (Mistral) via Ollama.


## Tools & Technologies Used

- **Python** – Core language for backend logic
- **Streamlit** – For creating the interactive UI
- **PyPDF2** – For reading and extracting text from PDFs
- **Ollama** – For running the Mistral model locally
- **Mistral** – Lightweight, high-performance LLM
- **Requests** – For making API calls to the local LLM server


## Project Workflow

### PDF Processing
- Extracted text from each uploaded PDF using `PyPDF2`
- Combined text across pages for querying

### Prompt Handling
- Captured user questions via the Streamlit input field
- Constructed prompts using both extracted text and user query

### Model Response
- Sent prompt to `http://localhost:11434/api/generate`
- Displayed the LLM's response in the Streamlit app interface


## User Interface Design

- Styled using a soft, modern color palette
- Clean layout with a sidebar for PDF uploads and a main content area for chat
- Responsive and minimal design using Streamlit's custom HTML/CSS injection


## Local Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/chat-with-pdf.git
cd chat-with-pdf
```

### 2. Install Python Dependencies
```bash
pip install -r requirements.txt
```

Or manually:

```bash
pip install streamlit PyPDF2 requests
```

### 3. Install and Setup Ollama
#### a. Download and install Ollama:  [Download Ollama](https://ollama.com/download)
#### b. Pull the Mistral model
```bash
ollama pull mistral
```
#### c. Start the Mistral model
```bash
ollama run mistral
```
This will launch the Mistral server.

# Run the App
Once Mistral is running, start the Streamlit app:

```bash
streamlit run app.py
```
Then open your browser and visit: http://localhost:8501

# Deployment
Note: This version is designed for offline/local use only and communicates with the locally running Mistral model via http://localhost:11434.

Deployment-based changes will be integrated soon so you can access the app from other devices or online.

# Requirements
```bash
pip install streamlit PyPDF2 requests
```
Ollama installed with the Mistral model:

```bash
ollama pull mistral
ollama run mistral
```
